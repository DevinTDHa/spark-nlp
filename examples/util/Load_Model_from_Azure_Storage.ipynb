{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8xIEZ07QpRM",
    "outputId": "03f475f3-1094-48cd-ff44-c30aa3f2c167"
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/util/Load_Model_from_Azure_Storage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E81LqDJzJIW7",
    "outputId": "967472fc-8ede-44ad-be4e-33a455ae7f36"
   },
   "source": [
    "## Loading Pretrained Models from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJiTfQKw90RZ",
    "outputId": "e31164d7-f26e-46db-a851-fe5e527dd29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK=3.4.0\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK=3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXU_LZZUJI6V",
    "outputId": "8974ebfa-1f03-4b9b-ca4d-97b066ca3726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m537.5/537.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade -q pyspark==$PYSPARK findspark spark_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KvNW4MU5rrF",
    "outputId": "661514cb-e45e-4f3c-f738-ebeda2cc7d47"
   },
   "source": [
    "## Defining Azure Storage URI in cache_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2xgzbVd7oqW"
   },
   "source": [
    "In Spark NLP you can configure the location to download the pre-trained models. Starting at Spark NLP 5.1.0, you can set a Azure Storage URI, GCP Storage URI or DBFS paths like HDFS or Databricks FS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX-TvMjn7oqW"
   },
   "source": [
    "In this notebook, we are going to see the steps required to use an external Azure Storage URI as `cache_pretrained` folder. To do this, we need to configure the spark session with the required settings for Spark NLP and Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQa6oC9W7oqX"
   },
   "source": [
    "### Spark NLP Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U_GdUbw7oqX"
   },
   "source": [
    "`cache_folder`: Here you must define your Azure URI that will store Spark NLP pre-trained models. This is defined in the config `spark.jsl.settings.pretrained.cache_folder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PzlikXd7oqY"
   },
   "source": [
    "### Spark ML Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7OnEu9t7oqY"
   },
   "source": [
    "Spark ML requires the following configuration to load a model from Azure:\n",
    "\n",
    "\n",
    "1. Azure connector: You need to identify your hadoop version and set the required dependency in `spark.jars.packages`\n",
    "2. Hadoop File System: You also need to setup the Hadoop file system to work with azure storage as file system. This is define in `spark.hadoop.fs.azure`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiKHRKxb7oqZ"
   },
   "source": [
    "To integrage with Azure, we need to define STORAGE_ACCOUNT and AZURE_ACCOUNT_KEY variables:\n",
    "1. STORAGE_ACCOUNT: This can be found in Microsoft Azure portal, in Resources look for the Type storage account and check the name that is your storage account.\n",
    "2. AZURE_ACCOUNT_KEY:\n",
    "Check View account access keys in this oficial [Azure documentation](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSQM36cd-dZm"
   },
   "source": [
    "## Loading Pretrained Models with `pretrained()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAa8Najj-mOV"
   },
   "source": [
    "You can define this two properties as variables to set those during spark session creation. In addition we also need to define the Azure container where the models are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r5kHoNhO7oqZ"
   },
   "outputs": [],
   "source": [
    "print(\"Enter your Storage Account:\")\n",
    "STORAGE_ACCOUNT = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qpibwTtJ7oqa"
   },
   "outputs": [],
   "source": [
    "print(\"Enter your Azure Account Key:\")\n",
    "AZURE_ACCOUNT_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PiDy32aC4BZ",
    "outputId": "01fb56ba-af95-48f7-99c7-a86380d1fb0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Azure Container:\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter your Azure Container:\")\n",
    "CONTAINER = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7M7I5OWB7oqa"
   },
   "outputs": [],
   "source": [
    "azure_hadoop_config = \"spark.hadoop.fs.azure.account.key.\" + STORAGE_ACCOUNT + \".blob.core.windows.net\"\n",
    "cache_folder = \"https://\" + STORAGE_ACCOUNT + \".blob.core.windows.net/\" + CONTAINER + \"/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZce9pGiU1NC",
    "outputId": "d38a06d9-22a8-4781-ac41-27b7cf195476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sparknlp2641242170.blob.core.windows.net/test/models\n"
     ]
    }
   ],
   "source": [
    "print(cache_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XLNO3Z9r6HgR"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3BPsat17oqb",
    "outputId": "b4e482fe-859f-4eda-fbad-b4799342704d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Apache Spark version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "hadoop_azure_pkg = \"org.apache.hadoop:hadoop-azure:3.3.4\"\n",
    "azure_storage_pkg = \"com.microsoft.azure:azure-storage:8.6.6\"\n",
    "azure_identity_pkg = \"com.azure:azure-identity:1.9.1\"\n",
    "azure_storage_blob_pkg = \"com.azure:azure-storage-blob:12.22.2\"\n",
    "azure_pkgs = hadoop_azure_pkg + \",\" + azure_storage_pkg + \",\" + azure_identity_pkg + \",\" + azure_storage_blob_pkg\n",
    "\n",
    "#Azure Storage configuration\n",
    "azure_params = {\n",
    "    \"spark.jars.packages\": azure_pkgs,\n",
    "    azure_hadoop_config: AZURE_ACCOUNT_KEY,\n",
    "    \"spark.jsl.settings.pretrained.cache_folder\": cache_folder\n",
    "}\n",
    "\n",
    "\n",
    "spark = sparknlp.start(real_time_output = True, params=azure_params)\n",
    "#spark = sparknlp.start(params=azure_params)\n",
    "\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMMvMHWK7oqb",
    "outputId": "401d2555-78eb-4a4d-d680-02d0d46479ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version = 3.3.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hadoop version = {spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWQ_qMXU7oqb"
   },
   "source": [
    "### Disclaimer:\n",
    "- Interaction with Azure depends on Spark/Hadoop/Azure implementations, which is out of our scope. Keep in mind that the configuration requirements or formats could change in other releases.\n",
    "- It's important to stand out that `hadoop-azure`, `azure-storage`, `azure_identity` and `azure-storage-blob` packages versions must be compatible. Otherwise, it won't work. The example of this notebook uses Spark 3.4.0 and Hadoop 3.3.4. So, you must modify those versions based on your Hadoop version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_eB72Yzg8_Jx"
   },
   "outputs": [],
   "source": [
    "sample_text = \"This is a sentence. This is another sentence\"\n",
    "data_df = spark.createDataFrame([[sample_text]]).toDF(\"text\").cache()\n",
    "\n",
    "empty_df = spark.createDataFrame([[\"\"]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tRyju8D-6XJ1"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5G4_BXwOYtC",
    "outputId": "d4b5b71e-3dcf-41f0-e23c-068728873862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[ / ]sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[ \\ ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "sentence_detector_dl = SentenceDetectorDLModel() \\\n",
    ".pretrained() \\\n",
    ".setInputCols([\"document\"]) \\\n",
    ".setOutputCol(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FhKPEMb09w6a"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector_dl, tokenizer])\n",
    "pipeline_model = pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CAp_AtrssPj",
    "outputId": "de05fff9-3cb6-464a-b72c-a8ac90b52fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|This is a sentenc...|[{document, 0, 43...|[{document, 0, 18...|[{token, 0, 3, Th...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline_model.transform(data_df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XX7puYR73Axk",
    "outputId": "d37b4cf7-b7dc-43cb-b204-089557a42b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9 MB\n",
      "[ / ]explain_document_ml download started this may take some time.\n",
      "Approximate size to download 9 MB\n",
      "[ — ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "pipeline_model = PretrainedPipeline('explain_document_ml', lang = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ifb9MFAeK4dC",
    "outputId": "f5ac8409-78f6-421b-a448-f5dc95885fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert_xlarge_token_classifier_conll03_pipeline download started this may take some time.\n",
      "Approx size to download 196.9 MB\n",
      "[ | ]albert_xlarge_token_classifier_conll03_pipeline download started this may take some time.\n",
      "Approximate size to download 196.9 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ner_chunk': ['John', 'John Snow Labs'],\n",
       " 'token': ['My',\n",
       "  'name',\n",
       "  'is',\n",
       "  'John',\n",
       "  'and',\n",
       "  'I',\n",
       "  'work',\n",
       "  'at',\n",
       "  'John',\n",
       "  'Snow',\n",
       "  'Labs',\n",
       "  '.'],\n",
       " 'sentence': ['My name is John and I work at John Snow Labs.']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline(\"albert_xlarge_token_classifier_conll03_pipeline\", lang = \"en\")\n",
    "pipeline.annotate(\"My name is John and I work at John Snow Labs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9KydCXh_DDZ"
   },
   "source": [
    "## Loading Pretrained Models with `load()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq39tJSLDWAy"
   },
   "source": [
    "Here we don't need to set `cache_folder`. So, you can ommit that configuration when starting a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jWDXsH8C_PAh"
   },
   "outputs": [],
   "source": [
    "model_path = \"wasbs://\" + CONTAINER + \"@\" + STORAGE_ACCOUNT + \".blob.core.windows.net/models/sentence_detector_dl_en_2.7.0_2.4_1609611052663/\"\n",
    "\n",
    "my_sentence_detector_dl = SentenceDetectorDLModel() \\\n",
    ".load(model_path) \\\n",
    ".setInputCols([\"document\"]) \\\n",
    ".setOutputCol(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Mga6iWZJBWsv"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[document_assembler, my_sentence_detector_dl, tokenizer])\n",
    "pipeline_model = pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GPbBdp8BYXU",
    "outputId": "0cfb7850-be39-471d-c2f6-4289b7c94070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|This is a sentenc...|[{document, 0, 43...|[{document, 0, 18...|[{token, 0, 3, Th...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline_model.transform(data_df)\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
